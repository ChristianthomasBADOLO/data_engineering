# Mon Parcours d'Apprentissage en Data Engineering

## À propos de moi
Je suis un(e) passionné(e) de technologie déterminé(e) à devenir un(e) data engineer compétent(e). Mon objectif est de maîtriser les outils et technologies essentiels du data engineering pour contribuer à des projets innovants et relever des défis data à grande échelle.

## Programme de formation
Je suis actuellement inscrit(e) à un programme de formation hybride en data engineering qui s'étend sur 10 semaines. Voici le plan détaillé avec suivi de progression :

<details>
<summary>Chapitre 1 - Introduction au Data Engineering et Bash + Vim (0/16)</summary>

- [*] Introduction au Data Engineering : Aperçu général et historique
- [*] Concepts clés : Différences entre Data Engineer, Data Scientist, Data Analyst, etc.
- [ ] Parcours professionnel en Data Engineering
- [*] Collaboration entre les différents rôles data
- [ ] Comparaison : Data Engineering vs Data Science
- [ ] Outils et utilisation des données en entreprise
- [ ] Évaluation de la maturité d'un projet Data
- [ ] Rôle de l'IA, des LLM et du Data Engineering
- [ ] Introduction à la modélisation des données
- [ ] Systèmes distribués : fonctionnement et avantages
- [ ] Scale Up vs Scale Out : stratégies de mise à l'échelle
- [ ] Design Pattern d'une plateforme de données
- [ ] Types de Data Pipelines : Batch et Streaming
- [ ] Gestion de la qualité des données
- [ ] Commandes Bash essentielles pour Windows/Mac
- [ ] Introduction à l'utilisation de Vim
</details>

<details>
<summary>Chapitre 2 - SQL (0/8)</summary>

- [ ] Installation et configuration de PostgreSQL et pgAdmin
- [ ] Concepts de base des SGBDR et types de données
- [ ] Requêtes SQL : SELECT, CREATE, ALTER TABLE, INSERT, UPDATE, DELETE, DROP
- [ ] Utilisation de NULL et requêtes conditionnelles (CASE)
- [ ] Jointures : JOIN, sous-requêtes, CTE, et opérations ensemblistes
- [ ] Travailler avec les dates et heures
- [ ] Fonctions de fenêtrage avancées
- [ ] Fonctions SQL : CAST, CONCAT, SUBSTRING, COALESCE, etc.
</details>

<details>
<summary>Chapitre 3 - Python (0/10)</summary>

- [ ] Introduction à Python et ses usages
- [ ] Installation de Python et configuration de l'environnement de développement
- [ ] Syntaxe de base et variables
- [ ] Types de données en Python
- [ ] Manipulation des données : conversion, chaînes, listes, tuples, sets, dictionnaires
- [ ] Opérateurs et mathématiques
- [ ] Contrôle de flux : conditions, boucles, compréhensions
- [ ] Fonctions : création, utilisation, lambda, décorateurs, gestion des erreurs
- [ ] Modules et objets
- [ ] Fonctions utiles et librairies : datetime, CSV, JSON, psycopg2
</details>

<details>
<summary>Chapitre 4 - Git et Github (0/13)</summary>

- [ ] Introduction à Git et GitFlow
- [ ] Installation et première utilisation de Git
- [ ] Fonctionnement interne de Git
- [ ] Gestion des fichiers : suppression, renommage, ignore
- [ ] Différence et historique des modifications
- [ ] Branches : création et gestion
- [ ] Merge : fusion de branches et gestion des conflits
- [ ] Utilisation de git stash
- [ ] Comparaison : GitHub vs GitLab vs AzureDevOps
- [ ] Processus pour pousser du code sur GitHub
- [ ] Pull Requests et organisation du code sur GitHub
- [ ] GitHub Flow : méthodologie de développement
- [ ] Synchronisation entre GitHub et le dépôt local
</details>

<details>
<summary>Chapitre 5 - Docker (0/10)</summary>

- [ ] Introduction à Docker et ses avantages pour le Data Engineering
- [ ] Installation de Docker
- [ ] Concepts de base de Docker
- [ ] Exécution du premier conteneur
- [ ] Fonctionnement interne de Docker
- [ ] Comparaison : Conteneur vs Machine Virtuelle
- [ ] Création de conteneurs avec Dockerfile
- [ ] Utilisation de Docker Compose pour la gestion multi-conteneurs
- [ ] Outils populaires de Data Engineering avec Docker
- [ ] Bonnes pratiques d'utilisation de Docker
</details>

<details>
<summary>Chapitre 6 - Airflow (0/10)</summary>

- [ ] Introduction à Apache Airflow et ses concepts de base
- [ ] Histoire et évolution d'Airflow
- [ ] Installation d'Airflow avec Docker
- [ ] Création et gestion des DAGs
- [ ] Exploration de l'interface utilisateur Airflow
- [ ] Architecture d'Airflow et ses composants principaux
- [ ] Configuration avancée avec airflow.cfg
- [ ] Création de processus ETL/ELT avec Airflow
- [ ] Concepts avancés et utilisation de la Taskflow API
- [ ] Meilleures pratiques pour l'utilisation d'Airflow
</details>

<details>
<summary>Chapitre 7 - Spark (0/12)</summary>

- [ ] Introduction à Apache Spark
- [ ] Histoire et évolution de Spark
- [ ] Importance de Spark pour les Data Engineers
- [ ] Composants principaux de Spark
- [ ] Installation de Spark avec Docker et Jupyter
- [ ] RDDs et DataFrames : concepts et opérations
- [ ] Utilisation de Spark SQL
- [ ] Gestion des différents types de fichiers avec Spark
- [ ] Développement et exécution d'applications Spark
- [ ] Fonctionnement interne de Spark
- [ ] Utilisation de Spark UI
- [ ] Meilleures pratiques pour l'optimisation de Spark
</details>

<details>
<summary>Chapitre 8 - Data Modeling et DuckDB (0/17)</summary>

- [ ] Introduction au Data Modeling
- [ ] Comparaison OLTP vs OLAP
- [ ] Exploration des systèmes OLTP
- [ ] Modèles de modélisation dimensionnelle pour OLAP
- [ ] Conception d'un Datalake
- [ ] Approches Kimball vs Inmon pour l'OLAP
- [ ] 5 étapes pour concevoir un Datawarehouse
- [ ] Introduction à DuckDB
- [ ] Installation et utilisation de DuckDB
- [ ] Création de tables de dimensions et de faits avec DuckDB
- [ ] Mise en place d'un star schema et analyses
- [ ] Techniques d'analyse avancée avec DuckDB
- [ ] Gestion des Slow Changing Dimensions (SCD)
- [ ] Normalisation et dénormalisation en OLAP
- [ ] Data Modeling dans le contexte du Big Data
- [ ] Concept de OneBigTable
- [ ] 14 meilleures pratiques en Data Modeling
</details>

<details>
<summary>Chapitre 9 - Power BI (0/6)</summary>

- [ ] Introduction à Power BI : fonctionnalités et avantages
- [ ] Installation de Power BI sur Windows
- [ ] Installation de Power BI sur Mac via Cloud Azure
- [ ] Connexion aux sources de données
- [ ] Création et exploration de dashboards
- [ ] Techniques d'exploration et d'analyse des données dans Power BI
</details>

## Objectifs d'apprentissage
- Maîtriser les fondamentaux du data engineering
- Développer des compétences avancées en SQL et Python
- Comprendre et utiliser efficacement les technologies de big data (Spark)
- Maîtriser les concepts de data warehousing, data lakes et data modeling
- Apprendre à créer et gérer des pipelines ETL avec Airflow
- Acquérir des compétences en conteneurisation avec Docker
- Développer des compétences en visualisation de données avec Power BI

## Progression générale du cours
Chapitres complétés : 0/9

## Progression par semaine
- [ ] Semaine 1 : Introduction au Data Engineering + CLI
- [ ] Semaine 2 : SQL avec PostgreSQL
- [ ] Semaine 3 : Python
- [ ] Semaine 4 : Git et Github
- [ ] Semaine 5 : Docker
- [ ] Semaine 6 : Airflow
- [ ] Semaine 7 : Spark
- [ ] Semaine 8 : Plateforme analytique et DuckDB
- [ ] Semaine 9 : Power BI